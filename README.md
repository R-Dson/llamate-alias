# Model Aliases for llamate

This file lists available model aliases and their default arguments.

You can add these models to your `llamate` setup using the commands shown.

## [hunyuan:a13b](https://huggingface.co/lmstudio-community/Hunyuan-A13B-Instruct-GGUF)
```bash
llamate add "hunyuan:a13b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- repeat-penalty: "1.05"
- top-k: "20"
- top-p:: "0.7"

</details>

## [code-nemotron:32b](https://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-32B-GGUF)
```bash
llamate add "code-nemotron:32b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [code-nemotron:14b](https://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-14B-GGUF)
```bash
llamate add "code-nemotron:14b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [code-nemotron:7b](https://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-7B-GGUF)
```bash
llamate add "code-nemotron:7b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B)
```bash
llamate add ""
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [phi4:reasoning-plus](https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF)
```bash
llamate add "phi4:reasoning-plus"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- jinja: ""
- temp: "0.8"
- top-p: "0.95"

</details>

## [llama3:8b](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF)
```bash
llamate add "llama3:8b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [llama3_1:8b](https://huggingface.co/unsloth/Llama-3.1-8B-Instruct-GGUF)
```bash
llamate add "llama3_1:8b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [llama3_2:1b](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF)
```bash
llamate add "llama3_2:1b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [llama3_2:3b](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF)
```bash
llamate add "llama3_2:3b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [llama3_3:70b](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF)
```bash
llamate add "llama3_3:70b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [llama3_1-nemotron-nano:8b](https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF)
```bash
llamate add "llama3_1-nemotron-nano:8b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [acereason-nemotron:14b](https://huggingface.co/unsloth/AceReason-Nemotron-14B-GGUF)
```bash
llamate add "acereason-nemotron:14b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- jinja: ""
- temp: "0.6"
- top-p: "0.95"

</details>

## [acereason-nemotron:7b](https://huggingface.co/bartowski/nvidia_AceReason-Nemotron-1.1-7B-GGUF)
```bash
llamate add "acereason-nemotron:7b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- jinja: ""
- temp: "0.6"
- top-p: "0.95"

</details>

## [llama3_3-nemotron-super:49b](https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF)
```bash
llamate add "llama3_3-nemotron-super:49b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [r1-distill:8b](https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF)
```bash
llamate add "r1-distill:8b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [magistral:small](https://huggingface.co/unsloth/Magistral-Small-2506-GGUF)
```bash
llamate add "magistral:small"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.7"
- top-p: "0.95"
- jinja: ""

</details>

## [mistral:small-3_1](https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF)
```bash
llamate add "mistral:small-3_1"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.15"

</details>

## [mistral:small-3_2](https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF)
```bash
llamate add "mistral:small-3_2"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.15"
- jinja: ""
- top-p: "1.00"

</details>

## [devstral:small](https://huggingface.co/unsloth/Devstral-Small-2505-GGUF)
```bash
llamate add "devstral:small"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [qwen3:32b](https://huggingface.co/unsloth/Qwen3-32B-GGUF)
```bash
llamate add "qwen3:32b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"
- top-k: "20"
- min-p: "0.0"

</details>

## [qwen3:30b-a3b](https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF)
```bash
llamate add "qwen3:30b-a3b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"
- top-k: "20"
- min-p: "0.0"

</details>

## [qwen3:14b](https://huggingface.co/unsloth/Qwen3-14B-GGUF)
```bash
llamate add "qwen3:14b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"
- top-k: "20"
- min-p: "0.0"

</details>

## [qwen3:8b](https://huggingface.co/unsloth/Qwen3-8B-GGUF)
```bash
llamate add "qwen3:8b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"
- top-k: "20"
- min-p: "0.0"

</details>

## [qwen3:4b](https://huggingface.co/unsloth/Qwen3-4B-GGUF)
```bash
llamate add "qwen3:4b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"
- top-k: "20"
- min-p: "0.0"

</details>

## [qwen3:1_7b](https://huggingface.co/unsloth/Qwen3-1.7B-GGUF)
```bash
llamate add "qwen3:1_7b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"
- top-k: "20"
- min-p: "0.0"

</details>

## [qwen3:0_6b](https://huggingface.co/unsloth/Qwen3-0.6B-GGUF)
```bash
llamate add "qwen3:0_6b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"
- top-k: "20"
- min-p: "0.0"

</details>

## [r1-distill-qwen:32b](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF)
```bash
llamate add "r1-distill-qwen:32b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"

</details>

## [gemma3:27b](https://huggingface.co/unsloth/gemma-3-27b-it-qat-GGUF)
```bash
llamate add "gemma3:27b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [gemma3:12b](https://huggingface.co/unsloth/gemma-3-12b-it-qat-GGUF)
```bash
llamate add "gemma3:12b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [gemma3:4b](https://huggingface.co/unsloth/gemma-3-4b-it-qat-GGUF)
```bash
llamate add "gemma3:4b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [gemma3:1b](https://huggingface.co/unsloth/gemma-3-1b-it-GGUF)
```bash
llamate add "gemma3:1b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [qwq:32b](https://huggingface.co/unsloth/QwQ-32B-GGUF)
```bash
llamate add "qwq:32b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- repeat-penalty: "1.1"
- dry-multiplier: "0.5"
- min-p: "0.01"
- top-k: "40"
- top-p: "0.95"
- samplers: "top_k;top_p;min_p;temperature;dry;typ_p;xtc"

</details>

## [jan-nano](https://huggingface.co/unsloth/Jan-nano-GGUF)
```bash
llamate add "jan-nano"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [jan-nano:q8](https://huggingface.co/unsloth/Jan-nano-GGUF)
```bash
llamate add "jan-nano:q8"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [seed-coder:8b](https://huggingface.co/unsloth/Seed-Coder-8B-Instruct-GGUF)
```bash
llamate add "seed-coder:8b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [seed-coder-reasoning:8b](https://huggingface.co/unsloth/Seed-Coder-8B-Reasoning-GGUF)
```bash
llamate add "seed-coder-reasoning:8b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [deepcoder-14b-preview](https://huggingface.co/bartowski/agentica-org_DeepCoder-14B-Preview-GGUF)
```bash
llamate add "deepcoder-14b-preview"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"
- temp: "0.6"
- top-p: "0.95"

</details>

## [skywork-swe:32b](https://huggingface.co/bartowski/Skywork_Skywork-SWE-32B-GGUF)
```bash
llamate add "skywork-swe:32b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [kimi-dev:72b](https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF)
```bash
llamate add "kimi-dev:72b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>

## [openbuddy-r1-distill-preview:32b](https://huggingface.co/bartowski/OpenBuddy_OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT-GGUF)
```bash
llamate add "openbuddy-r1-distill-preview:32b"
```
<details> <summary>parameters</summary>

- ctx-size: "8192"

</details>
